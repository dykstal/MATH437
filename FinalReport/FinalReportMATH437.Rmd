---
pdf_document: default
output: pdf_document
html_document: default
header-includes: \usepackage{amsmath,calc}
---

\begin{center}
\huge{Dank Title Here}

\large{Aidan Dykstal, Edward Hammond, \& Lilia James} \\
\large{May $4^{\text{th}}$, 2020}
\end{center}

```{r setup, include=FALSE}
# KnitR Setup for RMarkdown
knitr::opts_chunk$set(echo = TRUE)
```

# 1 \hspace{0.25cm} Introduction
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Introduce the Data & the Data Source             %
[//]: 2) Introduce Clear Research Questions               %
[//]: 3) Preview the Model that will be used to Solve     %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this project, we choose to focus on a dataset which includes information regarding people of the Pima Indian Tribe and their relationship to diabetes. The dataset contains 768 observations over 9 variables. The variables are as follow:

\begin{enumerate}

\item Number of times pregnant -
\item Plasma glucose concentration at two hours in oral glucose tolerance test
\item Diastolic blood pressure in mm/hg
\item Tricep skinfold thickness in mm
\item Two hour serum insulin concentration $\mu/ml$
\item Body Mass Index kg/$m^2$ 
\item Diabetes pedigree function
\item Age in years
\item Diabetic status - This categorical variable

\end{enumerate}
Research question: how accurately can we predict someone's diabetic status from the other variables? Do different classification models make a noticeable difference?

Models: QDA, LR, kNN.

\newpage

# 2 \hspace{0.25cm} The Quadratic Discriminant Analysis Approach
## 2.1 \hspace{0.25cm} Assumptions
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Clearly Explain Assumptions Needed for Validity  %
[//]: 2) Discuss Limitations of the Approach              %
[//]: 3) Preview the Checks of Assumptions/Limitations    %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES.

\newpage

## 2.2 \hspace{0.25cm} Model Construction Methods
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Describe Procedures to Construct the Model       %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES.

## 2.3 \hspace{0.25cm} Application to the STUDY-GOES-HERE 
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Explain Model's Answers to Research Questions    %
[//]: 2) Contextualize Assumptions and Limitations to     %
[//]:    the Problem of Interest                          %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES.

RESULTS:
```
   classPredictions
      0   1
  0 347 153
  1  52 216
[1] 0.2669271
   classPredictions
      0   1
  0 341 159
  1  60 208
[1] 0.2851562
```

## 2.4 \hspace{0.25cm} Model Limitations \& Appropriateness 
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Discuss the Limitations of this Approach as Seen %
[//]:    in the Results                                   %
[//]: 2) Carefully Check Model Assumptions and Validity:  %
[//]:    e.g., if Using Regression:                       %
[//]:       i) Thoroughly Examine Model Appropriateness   %
[//]:       ii) Plot Residuals vs. Fitted Values          %
[//]:       iii) Identify High-Leverage Points            %
[//]:       iv) Identify Influential Points. Fit the      %
[//]:           Model with/without Influential Points     %
[//]:       v) Identify any Outliers                      %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES.

\newpage

# 3 \hspace{0.25cm} The Logistic Regression Approach
## 3.1 \hspace{0.25cm} Assumptions
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Clearly Explain Assumptions Needed for Validity  %
[//]: 2) Discuss Limitations of the Approach              %
[//]: 3) Preview the Checks of Assumptions/Limitations    %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES. 

## 3.2 \hspace{0.25cm} Model Construction Methods
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Describe Procedures to Construct the Model       %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES.

## 3.3 \hspace{0.25cm} Application to the STUDY-GOES-HERE 
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Explain Model's Answers to Research Questions    %
[//]: 2) Contextualize Assumptions and Limitations to     %
[//]:    the Problem of Interest                          %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES.

RESULTS:
```
 class
      0   1
  0 445  55
  1 112 156
[1] 0.2174479
   pcv
      0   1
  0 443  57
  1 114 154
[1] 0.2226562
```

## 3.4 \hspace{0.25cm} Model Limitations \& Appropriateness 
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Discuss the Limitations of this Approach as Seen %
[//]:    in the Results                                   %
[//]: 2) Carefully Check Model Assumptions and Validity:  %
[//]:    e.g., if Using Regression:                       %
[//]:       i) Thoroughly Examine Model Appropriateness   %
[//]:       ii) Plot Residuals vs. Fitted Values          %
[//]:       iii) Identify High-Leverage Points            %
[//]:       iv) Identify Influential Points. Fit the      %
[//]:           Model with/without Influential Points     %
[//]:       v) Identify any Outliers                      %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES.

\newpage

# 4 \hspace{0.25cm} The $k$-Nearest Neighbors Approach
## 4.1 \hspace{0.25cm} Assumptions
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Clearly Explain Assumptions Needed for Validity  %
[//]: 2) Discuss Limitations of the Approach              %
[//]: 3) Preview the Checks of Assumptions/Limitations    %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES. 

## 4.2 \hspace{0.25cm} Model Construction Methods
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Describe Procedures to Construct the Model       %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES.

## 4.3 \hspace{0.25cm} Application to the STUDY-GOES-HERE 
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Explain Model's Answers to Research Questions    %
[//]: 2) Contextualize Assumptions and Limitations to     %
[//]:    the Problem of Interest                          %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES.

RESULTS:
```
[1] 21
   FinalFit
      0   1
  0 455  45
  1 116 152
[1] 0.2096354
   pcv
      1   2
  0 440  60
  1 126 142
[1] 0.2421875
```

## 4.4 \hspace{0.25cm} Model Limitations \& Appropriateness 
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Discuss the Limitations of this Approach as Seen %
[//]:    in the Results                                   %
[//]: 2) Carefully Check Model Assumptions and Validity:  %
[//]:    e.g., if Using Regression:                       %
[//]:       i) Thoroughly Examine Model Appropriateness   %
[//]:       ii) Plot Residuals vs. Fitted Values          %
[//]:       iii) Identify High-Leverage Points            %
[//]:       iv) Identify Influential Points. Fit the      %
[//]:           Model with/without Influential Points     %
[//]:       v) Identify any Outliers                      %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES.

\newpage

# 5 \hspace{0.25cm} Conclusions
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Create a Hollistic Summary of the Models and     %
[//]:    Compare them Qualitatively and Quantitatively    %
[//]: 2) If Appropriate, Choose a Superior Method and     %
[//]:    Review Results, Appropriateness, and Limitations %
[//]: 3) Draw Collective Conclusions between the Methods  %
[//]:    and Justify Results/Limitations/Implications by  %
[//]:    Referencing the Review of the Analysis.          %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DANKNUGGIES.

\newpage

# Appendix
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[//]: 1) Show R Code for All Computer Output/Results      %
[//]: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this appendix, we feature the \texttt{R} code used to generate the results and visualizations featured in this report. We will feature four primary sections for code: (1) data collection and cleaning, (2) STRATEGY-ONE model selection and analysis, (3) STRATEGY-TWO model selection and analysis, \& (4) STRATEGY-THREE model selection and analysis. 

## Data Collection \& Cleaning
SUMMARY. Comments in the code highlight specific tasks.
```{r, eval=FALSE}
# Collect the Pima Indian Diabetes Data into a Dataframe
BigChungus <- read.csv('Diabetes.csv', header = TRUE)
```

## Quadratic Discriminant Analysis
SUMMARY. Again, comments in the code reveal specific tasks. 
```{r, eval=FALSE}
# Bring in the MASS Library for QDA
library(MASS)

# Fit the QDA Model to the Diabetes Data
fitQDA <- qda(isDiabetic ~ .,
              data = BigChungus,
              prior = c(0.33, 0.67))
predictQDA <- predict(fitQDA, BigChungus)
classPredictions <- predictQDA$class

# Create the Confusion Matrix
Confusion <- table(BigChungus[,9],
                   classPredictions)
Confusion

# Estimate the Misclassification Rate
numCorrect <- sum(diag(Confusion))
numEstimated <- sum(Confusion)
accuracy <- numCorrect / numEstimated
misclassRate <- 1 - accuracy
misclassRate

# Now Use Leave-One-Out Cross Validation
n <- nrow(BigChungus)
classPredictions <- rep(0, n)
for (i in 1:n) {
  fitQDA <- qda(isDiabetic ~ .,
                data = BigChungus[-i,],
                prior = c(0.33, 0.67))
  predictQDA <- predict(fitQDA, BigChungus[i,])
  classPredictions[i] <- as.numeric(predictQDA$class) - 1
}

# Create the Confusion Matrix
Confusion <- table(BigChungus[,9],
                   classPredictions)
Confusion

# Estimate the Misclassification Rate
numCorrect <- sum(diag(Confusion))
numEstimated <- sum(Confusion)
accuracy <- numCorrect / numEstimated
misclassRate <- 1 - accuracy
misclassRate
```

\newpage

## Logistic Regression
SUMMARY. Again, comments in the code reveal specific tasks. 
```{r, eval=FALSE}
# Bring in the NNET Library for Logistic Regression
library(nnet)

# Begin Classifying by Logistic Regression
lrFit <- multinom(isDiabetic ~ .,
                  data = BigChungus,
                  trace = FALSE,
                  maxit = 10000)
coe <- summary(lrFit)$coefficients
LittleChungus <- BigChungus[-9]
nman <- t(LittleChungus)
logodds <- coe[1] + coe[-1] %*% nman
logodds <- cbind(0, t(logodds))
class <- apply(logodds, 1, which.max)
class <- class - 1

#Construct Confusion Matrix
Confusion <- table(BigChungus[,9],
                   class)
Confusion

# Estimate the Misclassification Rate
numCorrect <- sum(diag(Confusion))
numEstimated <- sum(Confusion)
accuracy <- numCorrect / numEstimated
misclassRate <- 1 - accuracy
misclassRate

# Now Try Leave-One-Out Cross-Validation
n <- length(BigChungus$isDiabetic)
pcv <- rep(0, n)

for (i in 1:n) {
  lrFit <- multinom(isDiabetic ~ .,
                    data = BigChungus[-i,],
                    trace = FALSE,
                    maxit = 10000)
  coe <- summary(lrFit)$coefficients
  tman <- t(BigChungus[i,-9])
  logodds <- coe[1] + coe[-1] %*% tman
  logodds <- cbind(0, t(logodds))
  pcv[i] <- which.max(logodds) - 1
}

#Construct Confusion Matrix
Confusion <- table(BigChungus[,9],pcv)
Confusion

# Estimate the Misclassification Rate
numCorrect <- sum(diag(Confusion))
numEstimated <- sum(Confusion)
accuracy <- numCorrect / numEstimated
misclassRate <- 1 - accuracy
misclassRate
```


\newpage

## $p$-Fold $k$-Nearest Neighbors
SUMMARY. Again, comments in the code reveal specific tasks. 
```{r, eval=FALSE}
# Load the Classifcation Library to Use kNN
library(class)

# Select Potential Values of k (for kNN) on a Log Scale
# Go from k = sqrt(10) to 10^{2.5}
kGrid <- 10^seq(0.5, 2.5, length.out = 20)
kGrid <- floor(kGrid)
MCR <- rep(0, length(kGrid))

# Find the Indices for the 10 Folds for p-Fold CV
n <- nrow(BigChungus)
p <- 10
folds <- sample(c(1:n),replace = FALSE)
foldInds <- seq(1, n, length.out = p + 1)
foldInds <- floor(foldInds)

# Estimate the Mean Misclassification Rate for Each Value
# of k in the kGrid for Each of the p Folds
for(i in 1:length(kGrid)){
  MCRS <- rep(0, p) 
  for(j in 1:p){
    testInds <- foldInds[j]:foldInds[j + 1]
    test <- BigChungus[testInds,]
    train <- BigChungus[-testInds,]
    fit <- knn(train,
               test,
               cl = train[,9],
               k = kGrid[i])
    
    # Construct Confusion Matrix
    Confusion <- table(test[,9], fit)

    # Estimate the Misclassification Rate
    right <- sum(diag(Confusion))
    total <- sum(Confusion)
    MCRS[j] <- 1 - (right/total)
  }
  
  # Get the Mean Misclassification Rate Over the Folds
  MCR[i] <- mean(MCRS)
}

# Select the Value of k (for kNN) that Minimizes the
# Mean Misclassification Rate Over the p Folds
theRightK <- which.min(MCR)
kBest <- kGrid[theRightK]
kBest
  
# Fit to the Entire Data Set with the Optimal k
FinalFit = knn(BigChungus,
               BigChungus,
               cl = BigChungus[,9],
               k = kBest)
Confusion <- table(BigChungus[,9], FinalFit)
Confusion

# Estimate the Optimal Misclassification Rate
right <- sum(diag(Confusion))
total <- sum(Confusion)
MCR <- 1 - (right/total)
MCR

# Fit with Leave-One-Out Cross Validation and Optimal k
pcv <- rep(0, n)
for (i in 1:n) {
  fit <- knn(BigChungus[-i,],
             BigChungus[i,],
             cl = BigChungus[-i, 9],
             k = kBest)
  pcv[i] <- fit
}

# Compute the Confusion Matrix
Confusion <- table(BigChungus[,9], pcv)
Confusion

# Estimate the Optimal Misclassification Rate
right <- sum(diag(Confusion))
total <- sum(Confusion)
MCR <- 1 - (right/total)
MCR
```